{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cifar10\n",
    "from ops import *\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TripleGan(object):\n",
    "    \n",
    "    def discriminator(self,x,label,scope='discriminator',is_training=True,reuse=False):\n",
    "        with tf.variable_scope(scope, reuse=reuse):\n",
    "            x=dropout(x,0.2)  #to prevent overfitting, probability is kept at 0.2\n",
    "            y=reshape(label,[-1,1,1,10])#reshaping label to a 4-D vector\n",
    "            x=conv_concat(x,y)#concatenating y to x by reshaping y to same dimensions as x and adding ones wherever necessary \n",
    "        \n",
    "        #convolution 1\n",
    "        x=conv_layer(x,filter_size=32,stride=1,kernel=[3,3])#convolution with stride=1 \n",
    "        x=lrelu(x,0.2)#adding rectified linear unit to prevent vanishing derivatives\n",
    "        x=conv_concat(x,y)#concatenating y for conditional discriminator so that we can condition it on basis of output as well 32*128\n",
    "        x=conv_layer(x,filter_size=32,stride=2,kernel=[3,3])#strides of 2 as x has now doubled because of conactenating y to it 32*64\n",
    "        x=dropout(x,0.2)\n",
    "        x=lrelu(x,0.2)\n",
    "        \n",
    "        \n",
    "        #convolution 2\n",
    "        x=conv_layer(x,filter_size=64,stride=1,kernel=[3,3])\n",
    "        x=lrelu(x,0.2)\n",
    "        x=conv_concat(x,y)\n",
    "        x=conv_layer(x,filter_size=64,stride=2,kernel=[3,3])\n",
    "        x=dropout(x,0.2)\n",
    "        x=lrelu(x,0.2) \n",
    "        \n",
    "        \n",
    "        #convolution 3\n",
    "        x=conv_layer(x,filter_size=64,stride=1,kernel=[3,3])\n",
    "        x=lrelu(x,0.2)\n",
    "        x=conv_concat(x,y)\n",
    "        x=conv_layer(x,filter_size=64,stride=2,kernel=[3,3])\n",
    "        x=lrelu(x,0.2) \n",
    "        \n",
    "        #FC layers\n",
    "        x=GAP(x)#global average pooling layer, used to prevent overfitting reduces dimensions of form h*w*d to form 1*1*d by taking averages of h and w values\n",
    "        x=flatten(x)#flattening to a 1-d array\n",
    "        x=concat(x,label)#concatenation of labels to the output after all convolution to the flattened layer,1st fully conneceted layer\n",
    "        logit=linear(x,unit=1)\n",
    "        output=sigmoid(logit)#activation function\n",
    "        \n",
    "        return output,logit,x\n",
    "    \n",
    "    \n",
    "    \n",
    "    def generator(self, z, y, scope='generator', is_training=True, reuse=False):\n",
    "        with tf.variable_scope(scope, reuse=reuse) :\n",
    "\n",
    "            x = concat([z, y]) # mlp_concat\n",
    "\n",
    "            x = relu(linear(x, unit=512*4*4, layer_name=scope+'_linear1'))\n",
    "            x = batch_norm(x, is_training=is_training, scope=scope+'_batch1')\n",
    "\n",
    "            x = tf.reshape(x, shape=[-1, 4, 4, 512])\n",
    "            y = tf.reshape(y, [-1, 1, 1, self.y_dim])\n",
    "            x = conv_concat(x,y)\n",
    "\n",
    "            x = relu(deconv_layer(x, filter_size=256, kernel=[5,5], stride=2, layer_name=scope+'_deconv1'))\n",
    "            x = batch_norm(x, is_training=is_training, scope=scope+'_batch2')\n",
    "            x = conv_concat(x,y)\n",
    "\n",
    "            x = relu(deconv_layer(x, filter_size=128, kernel=[5,5], stride=2, layer_name=scope+'_deconv2'))\n",
    "            x = batch_norm(x, is_training=is_training, scope=scope+'_batch3')\n",
    "            x = conv_concat(x,y)\n",
    "\n",
    "            x = tanh(deconv_layer(x, filter_size=3, kernel=[5,5], stride=2, wn=False, layer_name=scope+'deconv3'))\n",
    "\n",
    "        return x\n",
    "    \n",
    "    def classifier(self, x, scope='classifier', is_training=True, reuse=False):\n",
    "        with tf.variable_scope(scope, reuse=reuse) :\n",
    "            x = gaussian_noise_layer(x) # default = 0.15\n",
    "            x = lrelu(conv_layer(x, filter_size=128, kernel=[3,3], layer_name=scope+'_conv1'))\n",
    "            x = lrelu(conv_layer(x, filter_size=128, kernel=[3,3], layer_name=scope+'_conv2'))\n",
    "            x = lrelu(conv_layer(x, filter_size=128, kernel=[3,3], layer_name=scope+'_conv3'))\n",
    "\n",
    "            x = max_pooling(x, kernel=[2,2], stride=2)\n",
    "            x = dropout(x, rate=0.5, is_training=is_training)\n",
    "\n",
    "            x = lrelu(conv_layer(x, filter_size=256, kernel=[3,3], layer_name=scope+'_conv4'))\n",
    "            x = lrelu(conv_layer(x, filter_size=256, kernel=[3,3], layer_name=scope+'_conv5'))\n",
    "            x = lrelu(conv_layer(x, filter_size=256, kernel=[3,3], layer_name=scope+'_conv6'))\n",
    "\n",
    "            x = max_pooling(x, kernel=[2,2], stride=2)\n",
    "            x = dropout(x, rate=0.5, is_training=is_training)\n",
    "\n",
    "            x = lrelu(conv_layer(x, filter_size=512, kernel=[3,3], layer_name=scope+'_conv7'))\n",
    "            x = nin(x, unit=256, layer_name=scope+'_nin1')\n",
    "            x = nin(x, unit=128, layer_name=scope+'_nin2')\n",
    "\n",
    "            x = Global_Average_Pooling(x)\n",
    "            x = flatten(x)\n",
    "            x = linear(x, unit=10, layer_name=scope+'_linear1')\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
