{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'urllib2'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-fd7560bb98e0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mcifar10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Generative-Adversarial-Networks/Triple-GAN/cifar10.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mdownload\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mone_hot_encoded\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Generative-Adversarial-Networks/Triple-GAN/download.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0murllib2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRequest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtarfile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mzipfile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'urllib2'"
     ]
    }
   ],
   "source": [
    "import cifar10\n",
    "from ops import *\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TripleGan(object):\n",
    "    \n",
    "    def discriminator(self,x,label,scope='discriminator',is_training=True,reuse=False):\n",
    "        with tf.variable_scope(scope, reuse=reuse):\n",
    "            x=dropout(x,0.2)  #to prevent overfitting, probability is kept at 0.2\n",
    "            y=reshape(label,[-1,1,1,10])#reshaping label to a 4-D vector\n",
    "            x=conv_concat(x,y)#concatenating y to x by reshaping y to same dimensions as x and adding ones wherever necessary \n",
    "        \n",
    "        #convolution 1\n",
    "        x=conv_layer(x,filter_size=32,stride=1,kernel=[3,3])#convolution with stride=1 \n",
    "        x=lrelu(x,0.2)#adding rectified linear unit to prevent vanishing derivatives\n",
    "        x=conv_concat(x,y)#concatenating y for conditional discriminator so that we can condition it on basis of output as well 32*128\n",
    "        x=conv_layer(x,filter_size=32,stride=2,kernel=[3,3])#strides of 2 as x has now doubled because of conactenating y to it 32*64\n",
    "        x=dropout(x,0.2)\n",
    "        x=lrelu(x,0.2)\n",
    "        \n",
    "        \n",
    "        #convolution 2\n",
    "        x=conv_layer(x,filter_size=64,stride=1,kernel=[3,3])\n",
    "        x=lrelu(x,0.2)\n",
    "        x=conv_concat(x,y)\n",
    "        x=conv_layer(x,filter_size=64,stride=2,kernel=[3,3])\n",
    "        x=dropout(x,0.2)\n",
    "        x=lrelu(x,0.2) \n",
    "        \n",
    "        \n",
    "        #convolution 3\n",
    "        x=conv_layer(x,filter_size=64,stride=1,kernel=[3,3])\n",
    "        x=lrelu(x,0.2)\n",
    "        x=conv_concat(x,y)\n",
    "        x=conv_layer(x,filter_size=64,stride=2,kernel=[3,3])\n",
    "        x=lrelu(x,0.2) \n",
    "        \n",
    "        #FC layers\n",
    "        x=GAP(x)#global average pooling layer, used to prevent overfitting reduces dimensions of form h*w*d to form 1*1*d by taking averages of h and w values\n",
    "        x=flatten(x)#flattening to a 1-d array\n",
    "        x=concat(x,label)#concatenation of labels to the output after all convolution to the flattened layer,1st fully conneceted layer\n",
    "        logit=linear(x,unit=1)\n",
    "        output=sigmoid(logit)#activation function\n",
    "        \n",
    "        return output,logit,x\n",
    "    \n",
    "    \n",
    "    \n",
    "    def generator(self, z, y, scope='generator', is_training=True, reuse=False):\n",
    "        with tf.variable_scope(scope, reuse=reuse) :\n",
    "\n",
    "            x = concat([z, y]) # mlp_concat\n",
    "\n",
    "            x = relu(linear(x, unit=512*4*4, layer_name=scope+'_linear1'))\n",
    "            x = batch_norm(x, is_training=is_training, scope=scope+'_batch1')\n",
    "\n",
    "            x = tf.reshape(x, shape=[-1, 4, 4, 512])\n",
    "            y = tf.reshape(y, [-1, 1, 1, self.y_dim])\n",
    "            x = conv_concat(x,y)\n",
    "\n",
    "            x = relu(deconv_layer(x, filter_size=256, kernel=[5,5], stride=2, layer_name=scope+'_deconv1'))\n",
    "            x = batch_norm(x, is_training=is_training, scope=scope+'_batch2')\n",
    "            x = conv_concat(x,y)\n",
    "\n",
    "            x = relu(deconv_layer(x, filter_size=128, kernel=[5,5], stride=2, layer_name=scope+'_deconv2'))\n",
    "            x = batch_norm(x, is_training=is_training, scope=scope+'_batch3')\n",
    "            x = conv_concat(x,y)\n",
    "\n",
    "            x = tanh(deconv_layer(x, filter_size=3, kernel=[5,5], stride=2, wn=False, layer_name=scope+'deconv3'))\n",
    "\n",
    "        return x\n",
    "    \n",
    "    def classifier(self, x, scope='classifier', is_training=True, reuse=False):\n",
    "        with tf.variable_scope(scope, reuse=reuse) :\n",
    "            x = gaussian_noise_layer(x) # default = 0.15\n",
    "            x = lrelu(conv_layer(x, filter_size=128, kernel=[3,3], layer_name=scope+'_conv1'))\n",
    "            x = lrelu(conv_layer(x, filter_size=128, kernel=[3,3], layer_name=scope+'_conv2'))\n",
    "            x = lrelu(conv_layer(x, filter_size=128, kernel=[3,3], layer_name=scope+'_conv3'))\n",
    "\n",
    "            x = max_pooling(x, kernel=[2,2], stride=2)\n",
    "            x = dropout(x, rate=0.5, is_training=is_training)\n",
    "\n",
    "            x = lrelu(conv_layer(x, filter_size=256, kernel=[3,3], layer_name=scope+'_conv4'))\n",
    "            x = lrelu(conv_layer(x, filter_size=256, kernel=[3,3], layer_name=scope+'_conv5'))\n",
    "            x = lrelu(conv_layer(x, filter_size=256, kernel=[3,3], layer_name=scope+'_conv6'))\n",
    "\n",
    "            x = max_pooling(x, kernel=[2,2], stride=2)\n",
    "            x = dropout(x, rate=0.5, is_training=is_training)\n",
    "\n",
    "            x = lrelu(conv_layer(x, filter_size=512, kernel=[3,3], layer_name=scope+'_conv7'))\n",
    "            x = nin(x, unit=256, layer_name=scope+'_nin1')\n",
    "            x = nin(x, unit=128, layer_name=scope+'_nin2')\n",
    "\n",
    "            x = GAP(x)\n",
    "            x = flatten(x)\n",
    "            x = linear(x, unit=10, layer_name=scope+'_linear1')\n",
    "        return x\n",
    "    \n",
    "     def build_model(self):\n",
    "        image_dims = [self.input_height, self.input_width, self.c_dim]\n",
    "        bs = self.batch_size\n",
    "        unlabel_bs = self.unlabelled_batch_size\n",
    "        test_bs = self.test_batch_size\n",
    "        alpha = self.alpha\n",
    "        alpha_cla_adv = self.alpha_cla_adv\n",
    "        self.alpha_p = tf.placeholder(tf.float32, name='alpha_p')\n",
    "        self.gan_lr = tf.placeholder(tf.float32, name='gan_lr')\n",
    "        self.cla_lr = tf.placeholder(tf.float32, name='cla_lr')\n",
    "        self.unsup_weight = tf.placeholder(tf.float32, name='unsup_weight')\n",
    "        self.c_beta1 = tf.placeholder(tf.float32, name='c_beta1')\n",
    "\n",
    "        \"\"\" Graph Input \"\"\"\n",
    "        # images\n",
    "        self.inputs = tf.placeholder(tf.float32, [bs] + image_dims, name='real_images')\n",
    "        self.unlabelled_inputs = tf.placeholder(tf.float32, [unlabel_bs] + image_dims, name='unlabelled_images')\n",
    "        self.test_inputs = tf.placeholder(tf.float32, [test_bs] + image_dims, name='test_images')\n",
    "\n",
    "        # labels\n",
    "        self.y = tf.placeholder(tf.float32, [bs, self.y_dim], name='y')\n",
    "        self.unlabelled_inputs_y = tf.placeholder(tf.float32, [unlabel_bs, self.y_dim])\n",
    "        self.test_label = tf.placeholder(tf.float32, [test_bs, self.y_dim], name='test_label')\n",
    "        self.visual_y = tf.placeholder(tf.float32, [self.visual_num, self.y_dim], name='visual_y')\n",
    "\n",
    "        # noises\n",
    "        self.z = tf.placeholder(tf.float32, [bs, self.z_dim], name='z')\n",
    "        self.visual_z = tf.placeholder(tf.float32, [self.visual_num, self.z_dim], name='visual_z')\n",
    "\n",
    "        \"\"\" Loss Function \"\"\"\n",
    "        # A Game with Three Players\n",
    "\n",
    "        # output of D for real images\n",
    "        D_real, D_real_logits, _ = self.discriminator(self.inputs, self.y, is_training=True, reuse=False)\n",
    "\n",
    "        # output of D for fake images\n",
    "        G = self.generator(self.z, self.y, is_training=True, reuse=False)\n",
    "        D_fake, D_fake_logits, _ = self.discriminator(G, self.y, is_training=True, reuse=True)\n",
    "\n",
    "        # output of C for real images\n",
    "        C_real_logits = self.classifier(self.inputs, is_training=True, reuse=False)\n",
    "        R_L = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=self.y, logits=C_real_logits))\n",
    "\n",
    "        # output of D for unlabelled images\n",
    "        Y_c = self.classifier(self.unlabelled_inputs, is_training=True, reuse=True)\n",
    "        D_cla, D_cla_logits, _ = self.discriminator(self.unlabelled_inputs, Y_c, is_training=True, reuse=True)\n",
    "\n",
    "        # output of C for fake images\n",
    "        C_fake_logits = self.classifier(G, is_training=True, reuse=True)\n",
    "        R_P = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=self.y, logits=C_fake_logits))\n",
    "\n",
    "        #\n",
    "\n",
    "        # get loss for discriminator\n",
    "        d_loss_real = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_real_logits, labels=tf.ones_like(D_real)))\n",
    "        d_loss_fake = (1-alpha)*tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_fake_logits, labels=tf.zeros_like(D_fake)))\n",
    "        d_loss_cla = alpha*tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_cla_logits, labels=tf.zeros_like(D_cla)))\n",
    "        self.d_loss = d_loss_real + d_loss_fake + d_loss_cla\n",
    "\n",
    "        # get loss for generator\n",
    "        self.g_loss = (1-alpha)*tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_fake_logits, labels=tf.ones_like(D_fake)))\n",
    "\n",
    "        # test loss for classify\n",
    "        test_Y = self.classifier(self.test_inputs, is_training=False, reuse=True)\n",
    "        correct_prediction = tf.equal(tf.argmax(test_Y, 1), tf.argmax(self.test_label, 1))\n",
    "        self.accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "        # get loss for classify\n",
    "        max_c = tf.cast(tf.argmax(Y_c, axis=1), tf.float32)\n",
    "        c_loss_dis = tf.reduce_mean(max_c * tf.nn.softmax_cross_entropy_with_logits(logits=D_cla_logits, labels=tf.ones_like(D_cla)))\n",
    "        # self.c_loss = alpha * c_loss_dis + R_L + self.alpha_p*R_P\n",
    "\n",
    "        # R_UL = self.unsup_weight * tf.reduce_mean(tf.squared_difference(Y_c, self.unlabelled_inputs_y))\n",
    "        self.c_loss = alpha_cla_adv * alpha * c_loss_dis + R_L + self.alpha_p*R_P\n",
    "\n",
    "        \"\"\" Training \"\"\"\n",
    "\n",
    "        # divide trainable variables into a group for D and a group for G\n",
    "        t_vars = tf.trainable_variables()\n",
    "        d_vars = [var for var in t_vars if 'discriminator' in var.name]\n",
    "        g_vars = [var for var in t_vars if 'generator' in var.name]\n",
    "        c_vars = [var for var in t_vars if 'classifier' in var.name]\n",
    "\n",
    "        for var in t_vars: print(var.name)\n",
    "        # optimizers\n",
    "        with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)):\n",
    "            self.d_optim = tf.train.AdamOptimizer(self.gan_lr, beta1=self.GAN_beta1).minimize(self.d_loss, var_list=d_vars)\n",
    "            self.g_optim = tf.train.AdamOptimizer(self.gan_lr, beta1=self.GAN_beta1).minimize(self.g_loss, var_list=g_vars)\n",
    "            self.c_optim = tf.train.AdamOptimizer(self.cla_lr, beta1=self.beta1, beta2=self.beta2, epsilon=self.epsilon).minimize(self.c_loss, var_list=c_vars)\n",
    "\n",
    "        \"\"\"\" Testing \"\"\"\n",
    "        # for test\n",
    "        self.fake_images = self.generator(self.visual_z, self.visual_y, is_training=False, reuse=True)\n",
    "\n",
    "        \"\"\" Summary \"\"\"\n",
    "        d_loss_real_sum = tf.summary.scalar(\"d_loss_real\", d_loss_real)\n",
    "        d_loss_fake_sum = tf.summary.scalar(\"d_loss_fake\", d_loss_fake)\n",
    "        d_loss_cla_sum = tf.summary.scalar(\"d_loss_cla\", d_loss_cla)\n",
    "\n",
    "        d_loss_sum = tf.summary.scalar(\"d_loss\", self.d_loss)\n",
    "        g_loss_sum = tf.summary.scalar(\"g_loss\", self.g_loss)\n",
    "        c_loss_sum = tf.summary.scalar(\"c_loss\", self.c_loss)\n",
    "\n",
    "\n",
    "\n",
    "        # final summary operations\n",
    "        self.g_sum = tf.summary.merge([d_loss_fake_sum, g_loss_sum])\n",
    "        self.d_sum = tf.summary.merge([d_loss_real_sum, d_loss_sum])\n",
    "        self.c_sum = tf.summary.merge([d_loss_cla_sum, c_loss_sum])\n",
    "\n",
    "    def train(self):\n",
    "\n",
    "        # initialize all variables\n",
    "        tf.global_variables_initializer().run()\n",
    "        gan_lr = self.learning_rate\n",
    "        cla_lr = self.cla_learning_rate\n",
    "\n",
    "        # graph inputs for visualize training results\n",
    "        self.sample_z = np.random.uniform(-1, 1, size=(self.visual_num, self.z_dim))\n",
    "        self.test_codes = self.data_y[0:self.visual_num]\n",
    "\n",
    "        # saver to save model\n",
    "        self.saver = tf.train.Saver()\n",
    "\n",
    "        # summary writer\n",
    "        self.writer = tf.summary.FileWriter(self.log_dir + '/' + self.model_name, self.sess.graph)\n",
    "\n",
    "        # restore check-point if it exits\n",
    "        could_load, checkpoint_counter = self.load(self.checkpoint_dir)\n",
    "        if could_load:\n",
    "            start_epoch = (int)(checkpoint_counter / self.num_batches)\n",
    "            start_batch_id = checkpoint_counter - start_epoch * self.num_batches\n",
    "            counter = checkpoint_counter\n",
    "            with open('lr_logs.txt', 'r') as f :\n",
    "                line = f.readlines()\n",
    "                line = line[-1]\n",
    "                gan_lr = float(line.split()[0])\n",
    "                cla_lr = float(line.split()[1])\n",
    "                print(\"gan_lr : \", gan_lr)\n",
    "                print(\"cla_lr : \", cla_lr)\n",
    "            print(\" [*] Load SUCCESS\")\n",
    "        else:\n",
    "            start_epoch = 0\n",
    "            start_batch_id = 0\n",
    "            counter = 1\n",
    "            print(\" [!] Load failed...\")\n",
    "\n",
    "        # loop for epoch\n",
    "        start_time = time.time()\n",
    "\n",
    "        for epoch in range(start_epoch, self.epoch):\n",
    "\n",
    "            if epoch >= self.decay_epoch :\n",
    "                gan_lr *= 0.995\n",
    "                cla_lr *= 0.99\n",
    "                print(\"**** learning rate DECAY ****\")\n",
    "                print(gan_lr)\n",
    "                print(cla_lr)\n",
    "\n",
    "            if epoch >= self.apply_epoch :\n",
    "                alpha_p = self.apply_alpha_p\n",
    "            else :\n",
    "                alpha_p = self.init_alpha_p\n",
    "\n",
    "            rampup_value = rampup(epoch - 1)\n",
    "            unsup_weight = rampup_value * 100.0 if epoch > 1 else 0\n",
    "\n",
    "            # get batch data\n",
    "            for idx in range(start_batch_id, self.num_batches):\n",
    "                batch_images = self.data_X[idx * self.batch_size : (idx + 1) * self.batch_size]\n",
    "                batch_codes = self.data_y[idx * self.batch_size : (idx + 1) * self.batch_size]\n",
    "\n",
    "                batch_unlabelled_images = self.unlabelled_X[idx * self.unlabelled_batch_size : (idx + 1) * self.unlabelled_batch_size]\n",
    "                batch_unlabelled_images_y = self.unlabelled_y[idx * self.unlabelled_batch_size : (idx + 1) * self.unlabelled_batch_size]\n",
    "\n",
    "                batch_z = np.random.uniform(-1, 1, size=(self.batch_size, self.z_dim))\n",
    "\n",
    "                feed_dict = {\n",
    "                    self.inputs: batch_images, self.y: batch_codes,\n",
    "                    self.unlabelled_inputs: batch_unlabelled_images,\n",
    "                    self.unlabelled_inputs_y: batch_unlabelled_images_y,\n",
    "                    self.z: batch_z, self.alpha_p: alpha_p,\n",
    "                    self.gan_lr: gan_lr, self.cla_lr: cla_lr,\n",
    "                    self.unsup_weight : unsup_weight\n",
    "                }\n",
    "                # update D network\n",
    "                _, summary_str, d_loss = self.sess.run([self.d_optim, self.d_sum, self.d_loss], feed_dict=feed_dict)\n",
    "                self.writer.add_summary(summary_str, counter)\n",
    "\n",
    "                # update G network\n",
    "                _, summary_str_g, g_loss = self.sess.run([self.g_optim, self.g_sum, self.g_loss], feed_dict=feed_dict)\n",
    "                self.writer.add_summary(summary_str_g, counter)\n",
    "\n",
    "                # update C network\n",
    "                _, summary_str_c, c_loss = self.sess.run([self.c_optim, self.c_sum, self.c_loss], feed_dict=feed_dict)\n",
    "                self.writer.add_summary(summary_str_c, counter)\n",
    "\n",
    "                # display training status\n",
    "                counter += 1\n",
    "                print(\"Epoch: [%2d] [%4d/%4d] time: %4.4f, d_loss: %.8f, g_loss: %.8f, c_loss: %.8f\" \\\n",
    "                      % (epoch, idx, self.num_batches, time.time() - start_time, d_loss, g_loss, c_loss))\n",
    "\n",
    "                # save training results for every 100 steps\n",
    "                \"\"\"\n",
    "                if np.mod(counter, 100) == 0:\n",
    "                    samples = self.sess.run(self.fake_images,\n",
    "                                            feed_dict={self.z: self.sample_z, self.y: self.test_codes})\n",
    "                    image_frame_dim = int(np.floor(np.sqrt(self.visual_num)))\n",
    "                    save_images(samples[:image_frame_dim * image_frame_dim, :, :, :], [image_frame_dim, image_frame_dim],\n",
    "                                './' + check_folder(\n",
    "                                    self.result_dir + '/' + self.model_dir) + '/' + self.model_name + '_train_{:02d}_{:04d}.png'.format(\n",
    "                                    epoch, idx))\n",
    "                \"\"\"\n",
    "\n",
    "            # classifier test\n",
    "            test_acc = 0.0\n",
    "\n",
    "            for idx in range(10) :\n",
    "                test_batch_x = self.test_X[idx * self.test_batch_size : (idx+1) * self.test_batch_size]\n",
    "                test_batch_y = self.test_y[idx * self.test_batch_size : (idx+1) * self.test_batch_size]\n",
    "\n",
    "                acc_ = self.sess.run(self.accuracy, feed_dict={\n",
    "                    self.test_inputs: test_batch_x,\n",
    "                    self.test_label: test_batch_y\n",
    "                })\n",
    "\n",
    "                test_acc += acc_\n",
    "            test_acc /= 10\n",
    "\n",
    "            summary_test = tf.Summary(value=[tf.Summary.Value(tag='test_accuracy', simple_value=test_acc)])\n",
    "            self.writer.add_summary(summary_test, epoch)\n",
    "\n",
    "            line = \"Epoch: [%2d], test_acc: %.4f\\n\" % (epoch, test_acc)\n",
    "            print(line)\n",
    "            lr = \"{} {}\".format(gan_lr, cla_lr)\n",
    "            with open('logs.txt', 'a') as f:\n",
    "                f.write(line)\n",
    "            with open('lr_logs.txt', 'a') as f :\n",
    "                f.write(lr+'\\n')\n",
    "\n",
    "            # After an epoch, start_batch_id is set to zero\n",
    "            # non-zero value is only for the first epoch after loading pre-trained model\n",
    "            start_batch_id = 0\n",
    "\n",
    "            # save model\n",
    "            self.save(self.checkpoint_dir, counter)\n",
    "\n",
    "            # show temporal results\n",
    "            self.visualize_results(epoch)\n",
    "\n",
    "            # save model for final step\n",
    "        self.save(self.checkpoint_dir, counter)\n",
    "\n",
    "    def visualize_results(self, epoch):\n",
    "        # tot_num_samples = min(self.sample_num, self.batch_size)\n",
    "        image_frame_dim = int(np.floor(np.sqrt(self.visual_num)))\n",
    "        z_sample = np.random.uniform(-1, 1, size=(self.visual_num, self.z_dim))\n",
    "\n",
    "        \"\"\" random noise, random discrete code, fixed continuous code \"\"\"\n",
    "        y = np.random.choice(self.len_discrete_code, self.visual_num)\n",
    "        # Generated 10 labels with batch_size\n",
    "        y_one_hot = np.zeros((self.visual_num, self.y_dim))\n",
    "        y_one_hot[np.arange(self.visual_num), y] = 1\n",
    "\n",
    "        samples = self.sess.run(self.fake_images, feed_dict={self.visual_z: z_sample, self.visual_y: y_one_hot})\n",
    "\n",
    "        save_images(samples[:image_frame_dim * image_frame_dim, :, :, :], [image_frame_dim, image_frame_dim],\n",
    "                    check_folder(\n",
    "                        self.result_dir + '/' + self.model_dir + '/all_classes') + '/' + self.model_name + '_epoch%03d' % epoch + '_test_all_classes.png')\n",
    "\n",
    "        \"\"\" specified condition, random noise \"\"\"\n",
    "        n_styles = 10  # must be less than or equal to self.batch_size\n",
    "\n",
    "        np.random.seed()\n",
    "        si = np.random.choice(self.visual_num, n_styles)\n",
    "\n",
    "        for l in range(self.len_discrete_code):\n",
    "            y = np.zeros(self.visual_num, dtype=np.int64) + l\n",
    "            y_one_hot = np.zeros((self.visual_num, self.y_dim))\n",
    "            y_one_hot[np.arange(self.visual_num), y] = 1\n",
    "\n",
    "            samples = self.sess.run(self.fake_images, feed_dict={self.visual_z: z_sample, self.visual_y: y_one_hot})\n",
    "            save_images(samples[:image_frame_dim * image_frame_dim, :, :, :], [image_frame_dim, image_frame_dim],\n",
    "                        check_folder(\n",
    "                            self.result_dir + '/' + self.model_dir + '/class_%d' % l) + '/' + self.model_name + '_epoch%03d' % epoch + '_test_class_%d.png' % l)\n",
    "\n",
    "            samples = samples[si, :, :, :]\n",
    "\n",
    "            if l == 0:\n",
    "                all_samples = samples\n",
    "            else:\n",
    "                all_samples = np.concatenate((all_samples, samples), axis=0)\n",
    "\n",
    "        \"\"\" save merged images to check style-consistency \"\"\"\n",
    "        canvas = np.zeros_like(all_samples)\n",
    "        for s in range(n_styles):\n",
    "            for c in range(self.len_discrete_code):\n",
    "                canvas[s * self.len_discrete_code + c, :, :, :] = all_samples[c * n_styles + s, :, :, :]\n",
    "\n",
    "        save_images(canvas, [n_styles, self.len_discrete_code],\n",
    "                    check_folder(\n",
    "                        self.result_dir + '/' + self.model_dir + '/all_classes_style_by_style') + '/' + self.model_name + '_epoch%03d' % epoch + '_test_all_classes_style_by_style.png')\n",
    "\n",
    "    @property\n",
    "    def model_dir(self):\n",
    "        return \"{}_{}_{}_{}\".format(\n",
    "            self.model_name, self.dataset_name,\n",
    "            self.batch_size, self.z_dim)\n",
    "\n",
    "    def save(self, checkpoint_dir, step):\n",
    "        checkpoint_dir = os.path.join(checkpoint_dir, self.model_dir, self.model_name)\n",
    "\n",
    "        if not os.path.exists(checkpoint_dir):\n",
    "            os.makedirs(checkpoint_dir)\n",
    "\n",
    "        self.saver.save(self.sess, os.path.join(checkpoint_dir, self.model_name + '.model'), global_step=step)\n",
    "\n",
    "    def load(self, checkpoint_dir):\n",
    "        import re\n",
    "        print(\" [*] Reading checkpoints...\")\n",
    "        checkpoint_dir = os.path.join(checkpoint_dir, self.model_dir, self.model_name)\n",
    "\n",
    "        ckpt = tf.train.get_checkpoint_state(checkpoint_dir)\n",
    "        if ckpt and ckpt.model_checkpoint_path:\n",
    "            ckpt_name = os.path.basename(ckpt.model_checkpoint_path)\n",
    "            self.saver.restore(self.sess, os.path.join(checkpoint_dir, ckpt_name))\n",
    "            counter = int(next(re.finditer(\"(\\d+)(?!.*\\d)\", ckpt_name)).group(0))\n",
    "            print(\" [*] Success to read {}\".format(ckpt_name))\n",
    "            return True, counter\n",
    "        else:\n",
    "            print(\" [*] Failed to find a checkpoint\")\n",
    "        return False, 0\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "newkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
